{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwjGUDr7G6b6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ix9OmKRG6ZW"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbcZLrKGG6Ws"
      },
      "source": [
        "torch.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vjX-51MG6T7"
      },
      "source": [
        "pip install torchvision==0.4.0 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ft6kvaAnG6RJ"
      },
      "source": [
        "!pip install torch==1.4.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDcYtYPWJGTy"
      },
      "source": [
        "!pip install advertorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WhaRbsfG6Of"
      },
      "source": [
        "'''\n",
        "Properly implemented ResNet-s for CIFAR10 as described in paper [1].\n",
        "The implementation and structure of this file is hugely influenced by [2]\n",
        "which is implemented for ImageNet and doesn't have option A for identity.\n",
        "Moreover, most of the implementations on the web is copy-paste from\n",
        "torchvision's resnet and has wrong number of params.\n",
        "Proper ResNet-s for CIFAR10 (for fair comparision and etc.) has following\n",
        "number of layers and parameters:\n",
        "name      | layers | params\n",
        "ResNet20  |    20  | 0.27M\n",
        "ResNet32  |    32  | 0.46M\n",
        "ResNet44  |    44  | 0.66M\n",
        "ResNet56  |    56  | 0.85M\n",
        "ResNet110 |   110  |  1.7M\n",
        "ResNet1202|  1202  | 19.4m\n",
        "which this implementation indeed has.\n",
        "Reference:\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
        "[2] https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
        "If you use this implementation in you work, please don't forget to mention the\n",
        "author, Yerlan Idelbayev.\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from advertorch.utils import NormalizeByChannelMeanStd\n",
        "\n",
        "__all__ = ['ResNet', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110', 'resnet1202']\n",
        "\n",
        "\n",
        "def _weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    #print(classname)\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight)\n",
        "\n",
        "class LambdaLayer(nn.Module):\n",
        "    def __init__(self, lambd):\n",
        "        super(LambdaLayer, self).__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            if option == 'A':\n",
        "                \"\"\"\n",
        "                For CIFAR10 ResNet paper uses option A.\n",
        "                \"\"\"\n",
        "                self.shortcut = LambdaLayer(lambda x:\n",
        "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
        "            elif option == 'B':\n",
        "                self.shortcut = nn.Sequential(\n",
        "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                     nn.BatchNorm2d(self.expansion * planes)\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.normalize = NormalizeByChannelMeanStd(\n",
        "            mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616])\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "        self.apply(_weights_init)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.normalize(x)\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, out.size()[3])\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def resnet20(number_class=10):\n",
        "    return ResNet(BasicBlock, [3, 3, 3], num_classes=number_class)\n",
        "\n",
        "\n",
        "def resnet32(number_class=10):\n",
        "    return ResNet(BasicBlock, [5, 5, 5], num_classes=number_class)\n",
        "\n",
        "\n",
        "def resnet44(number_class=10):\n",
        "    return ResNet(BasicBlock, [7, 7, 7], num_classes=number_class)\n",
        "\n",
        "\n",
        "def resnet56(number_class=10):\n",
        "    return ResNet(BasicBlock, [9, 9, 9], num_classes=number_class)\n",
        "\n",
        "\n",
        "def resnet110(number_class=10):\n",
        "    return ResNet(BasicBlock, [18, 18, 18], num_classes=number_class)\n",
        "\n",
        "\n",
        "def resnet1202(number_class=10):\n",
        "    return ResNet(BasicBlock, [200, 200, 200], num_classes=number_class)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5uh67iBJL1C"
      },
      "source": [
        "model = resnet20(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDy9_WQhG6L1"
      },
      "source": [
        "PATH =\"15model_SA_best.pth.tar\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4gWXO_5G6JM"
      },
      "source": [
        "checkpoint = torch.load(PATH,map_location=torch.device('cpu'))\n",
        "checkpoint.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG9nlNG2G6GT"
      },
      "source": [
        "model.load_state_dict(checkpoint['state_dict'],strict=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_b10GJYlG6Dt"
      },
      "source": [
        "#model = model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7ai0KZRG6A4"
      },
      "source": [
        "checkpoint = torch.load(PATH, map_location = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qmYYDJvG565"
      },
      "source": [
        "best_sa = checkpoint['best_sa']\n",
        "start_epoch = checkpoint['epoch']\n",
        "\n",
        "all_result = checkpoint['result']\n",
        "start_state = checkpoint['state']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sLeVw0KMfr7"
      },
      "source": [
        "def check_sparsity(model):\n",
        "    \n",
        "    sum_list = 0\n",
        "    zero_sum = 0\n",
        "\n",
        "    for name,m in model.named_modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            sum_list = sum_list+float(m.weight.nelement())\n",
        "            zero_sum = zero_sum+float(torch.sum(m.weight == 0))  \n",
        "\n",
        "    print('* remain weight = ', 100*(1-zero_sum/sum_list),'%')\n",
        "    return 100*(1-zero_sum/sum_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHVLN0BkMfpa"
      },
      "source": [
        "print(check_sparsity(model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isOkZssvMfjR"
      },
      "source": [
        "for k in checkpoint['state_dict'].keys():\n",
        "  if 'mask' in k:\n",
        "    print(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E788jBi3Mwl4"
      },
      "source": [
        "import copy \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils.prune as prune"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BGSJn1mMwjh"
      },
      "source": [
        "def extract_mask(model_dict):\n",
        "\n",
        "    new_dict = {}\n",
        "\n",
        "    for key in model_dict.keys():\n",
        "        if 'mask' in key:\n",
        "            new_dict[key] = copy.deepcopy(model_dict[key])\n",
        "    return new_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unrljG15Mwg0"
      },
      "source": [
        "current_mask = extract_mask(checkpoint['state_dict'])\n",
        "current_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QQfVTs1MweK"
      },
      "source": [
        "def prune_model_custom(model, mask_dict):\n",
        "\n",
        "    print('Pruning with custom mask')\n",
        "    for name,m in model.named_modules():\n",
        "        # print(name, m)\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            prune.CustomFromMask.apply(m, 'weight', mask=mask_dict[name+'.weight_mask'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wU80utNhMwbp"
      },
      "source": [
        "prune_model_custom(model, current_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK0QGPTVMwY3"
      },
      "source": [
        "model.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3GXo0IyOXEP"
      },
      "source": [
        "# Convert pytorch model to Tensorflow model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TKzflAVOeyV"
      },
      "source": [
        "pip install onnx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXBQv6RaOewF"
      },
      "source": [
        "!pip install tensorflow-addons\n",
        "!git clone https://github.com/onnx/onnx-tensorflow.git && cd onnx-tensorflow && pip install -e . "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6n2I_tBQVHB"
      },
      "source": [
        "%cd './onnx-tensorflow'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUIpNXqzOeth"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "import onnx\n",
        "from onnx_tf.backend import prepare"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxnIk_POOerA"
      },
      "source": [
        "%cd '..'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3cBjMBdOeoi"
      },
      "source": [
        " dummy_input = Variable(torch.randn(1, 1, 28, 28))\n",
        " torch.onnx.export(model, dummy_input, \"mnist.onnx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jCcn8VxMwWX"
      },
      "source": [
        "#https://analyticsindiamag.com/converting-a-model-from-pytorch-to-tensorflow-guide-to-onnx/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDv_XtgLg0Kc"
      },
      "source": [
        "!pip3 install pytorch2keras\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FCf7UvMg3K8"
      },
      "source": [
        "def pytorch_to_keras(\n",
        "    model, args, input_shapes=None,\n",
        "    change_ordering=False, verbose=False, name_policy=None,\n",
        "    use_optimizer=False, do_constant_folding=False):\n",
        "    onnx_model = onnx.load(stream)\n",
        "    k_model = onnx_to_keras(onnx_model=onnx_model, input_names=input_names,\n",
        "                            input_shapes=input_shapes, name_policy=name_policy,\n",
        "                            verbose=verbose, change_ordering=change_ordering)\n",
        "    return k_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHJ92oXMhIpo"
      },
      "source": [
        "!pip install onnx==1.8.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDoPXmwbhmCL"
      },
      "source": [
        "\n",
        "from pytorch2keras.converter import pytorch_to_keras\n",
        "\n",
        "def converted_fully_convolutional_resnet18(\n",
        "    input_tensor, pretrained_resnet=True,):\n",
        "    # define input tensor\n",
        "    input_var = Variable(torch.FloatTensor(input_tensor))\n",
        "    # get PyTorch ResNet18 model\n",
        "    model_to_transfer = FullyConvolutionalResnet18(pretrained=pretrained_resnet)\n",
        "    model_to_transfer.eval()\n",
        "    # convert PyTorch model to Keras\n",
        "    model = pytorch_to_keras(\n",
        "        model_to_transfer,\n",
        "        input_var,\n",
        "        [input_var.shape[-3:]],\n",
        "        change_ordering=True,\n",
        "        verbose=False,\n",
        "        name_policy=\"keep\",)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG6WZwTeio8j"
      },
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "summary(model, input_size=input_var.shape[-3:])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDstbplC-AHF"
      },
      "source": [
        "# Knowledge Distillation\n",
        "\n",
        "**Author:** [Kenneth Borup](https://twitter.com/Kennethborup)<br>\n",
        "**Date created:** 2020/09/01<br>\n",
        "**Last modified:** 2020/09/01<br>\n",
        "**Description:** Implementation of classical Knowledge Distillation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ljj8l4bQ-AHH"
      },
      "source": [
        "## Introduction to Knowledge Distillation\n",
        "\n",
        "Knowledge Distillation is a procedure for model\n",
        "compression, in which a small (student) model is trained to match a large pre-trained\n",
        "(teacher) model. Knowledge is transferred from the teacher model to the student\n",
        "by minimizing a loss function, aimed at matching softened teacher logits as well as\n",
        "ground-truth labels.\n",
        "\n",
        "The logits are softened by applying a \"temperature\" scaling function in the softmax,\n",
        "effectively smoothing out the probability distribution and revealing\n",
        "inter-class relationships learned by the teacher.\n",
        "\n",
        "**Reference:**\n",
        "\n",
        "- [Hinton et al. (2015)](https://arxiv.org/abs/1503.02531)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z2R2I2H-AHI"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8ZE4-JL-AHI"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhfGxGyE-AHJ"
      },
      "source": [
        "## Construct `Distiller()` class\n",
        "\n",
        "The custom `Distiller()` class, overrides the `Model` methods `train_step`, `test_step`,\n",
        "and `compile()`. In order to use the distiller, we need:\n",
        "\n",
        "- A trained teacher model\n",
        "- A student model to train\n",
        "- A student loss function on the difference between student predictions and ground-truth\n",
        "- A distillation loss function, along with a `temperature`, on the difference between the\n",
        "soft student predictions and the soft teacher labels\n",
        "- An `alpha` factor to weight the student and distillation loss\n",
        "- An optimizer for the student and (optional) metrics to evaluate performance\n",
        "\n",
        "In the `train_step` method, we perform a forward pass of both the teacher and student,\n",
        "calculate the loss with weighting of the `student_loss` and `distillation_loss` by `alpha` and\n",
        "`1 - alpha`, respectively, and perform the backward pass. Note: only the student weights are updated,\n",
        "and therefore we only calculate the gradients for the student weights.\n",
        "\n",
        "In the `test_step` method, we evaluate the student model on the provided dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-QsfZLq-AHJ"
      },
      "source": [
        "\n",
        "class Distiller(keras.Model):\n",
        "    def __init__(self, student, teacher):\n",
        "        super(Distiller, self).__init__()\n",
        "        self.teacher = teacher\n",
        "        self.student = student\n",
        "\n",
        "    def compile(\n",
        "        self,\n",
        "        optimizer,\n",
        "        metrics,\n",
        "        student_loss_fn,\n",
        "        distillation_loss_fn,\n",
        "        alpha=0.1,\n",
        "        temperature=3,\n",
        "    ):\n",
        "        \"\"\" Configure the distiller.\n",
        "\n",
        "        Args:\n",
        "            optimizer: Keras optimizer for the student weights\n",
        "            metrics: Keras metrics for evaluation\n",
        "            student_loss_fn: Loss function of difference between student\n",
        "                predictions and ground-truth\n",
        "            distillation_loss_fn: Loss function of difference between soft\n",
        "                student predictions and soft teacher predictions\n",
        "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
        "            temperature: Temperature for softening probability distributions.\n",
        "                Larger temperature gives softer distributions.\n",
        "        \"\"\"\n",
        "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
        "        self.student_loss_fn = student_loss_fn\n",
        "        self.distillation_loss_fn = distillation_loss_fn\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Unpack data\n",
        "        x, y = data\n",
        "\n",
        "        # Forward pass of teacher\n",
        "        teacher_predictions = self.teacher(x, training=False)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass of student\n",
        "            student_predictions = self.student(x, training=True)\n",
        "\n",
        "            # Compute losses\n",
        "            student_loss = self.student_loss_fn(y, student_predictions)\n",
        "            distillation_loss = self.distillation_loss_fn(\n",
        "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
        "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
        "            )\n",
        "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.student.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Update the metrics configured in `compile()`.\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\n",
        "\n",
        "        # Return a dict of performance\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update(\n",
        "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
        "        )\n",
        "        return results\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "\n",
        "        # Compute predictions\n",
        "        y_prediction = self.student(x, training=False)\n",
        "\n",
        "        # Calculate the loss\n",
        "        student_loss = self.student_loss_fn(y, y_prediction)\n",
        "\n",
        "        # Update the metrics.\n",
        "        self.compiled_metrics.update_state(y, y_prediction)\n",
        "\n",
        "        # Return a dict of performance\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update({\"student_loss\": student_loss})\n",
        "        return results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiprVxcH-AHK"
      },
      "source": [
        "## Create student and teacher models\n",
        "\n",
        "Initialy, we create a teacher model and a smaller student model. Both models are\n",
        "convolutional neural networks and created using `Sequential()`,\n",
        "but could be any Keras model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-JQ1PTg-AHK"
      },
      "source": [
        "# Create the teacher\n",
        "teacher = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(28, 28, 1)),\n",
        "        layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
        "        layers.Conv2D(512, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(10),\n",
        "    ],\n",
        "    name=\"teacher\",\n",
        ")\n",
        "\n",
        "# Create the student\n",
        "student = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(28, 28, 1)),\n",
        "        layers.Conv2D(16, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
        "        layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(10),\n",
        "    ],\n",
        "    name=\"student\",\n",
        ")\n",
        "\n",
        "# Clone student for later comparison\n",
        "student_scratch = keras.models.clone_model(student)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOuXvSvq-AHL"
      },
      "source": [
        "## Prepare the dataset\n",
        "\n",
        "The dataset used for training the teacher and distilling the teacher is\n",
        "[MNIST](https://keras.io/api/datasets/mnist/), and the procedure would be equivalent for any other\n",
        "dataset, e.g. [CIFAR-10](https://keras.io/api/datasets/cifar10/), with a suitable choice\n",
        "of models. Both the student and teacher are trained on the training set and evaluated on\n",
        "the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAyegH3g-AHL"
      },
      "source": [
        "# Prepare the train and test dataset.\n",
        "batch_size = 64\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize data\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_train = np.reshape(x_train, (-1, 28, 28, 1))\n",
        "\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "x_test = np.reshape(x_test, (-1, 28, 28, 1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoiyqgU1-AHL"
      },
      "source": [
        "## Train the teacher\n",
        "\n",
        "In knowledge distillation we assume that the teacher is trained and fixed. Thus, we start\n",
        "by training the teacher model on the training set in the usual way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqehIagz-AHM"
      },
      "source": [
        "# Train teacher as usual\n",
        "teacher.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "# Train and evaluate teacher on data.\n",
        "teacher.fit(x_train, y_train, epochs=10)\n",
        "teacher.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MlFhNZQ-AHM"
      },
      "source": [
        "## Distill teacher to student\n",
        "\n",
        "We have already trained the teacher model, and we only need to initialize a\n",
        "`Distiller(student, teacher)` instance, `compile()` it with the desired losses,\n",
        "hyperparameters and optimizer, and distill the teacher to the student."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2KnEvRE-AHM"
      },
      "source": [
        "# Initialize and compile distiller\n",
        "distiller = Distiller(student=student, teacher=teacher)\n",
        "distiller.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
        "    alpha=0.1,\n",
        "    temperature=10,\n",
        ")\n",
        "\n",
        "# Distill teacher to student\n",
        "distiller.fit(x_train, y_train, epochs=10)\n",
        "\n",
        "# Evaluate student on test dataset\n",
        "distiller.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsSnOQsr-AHM"
      },
      "source": [
        "## Train student from scratch for comparison\n",
        "\n",
        "We can also train an equivalent student model from scratch without the teacher, in order\n",
        "to evaluate the performance gain obtained by knowledge distillation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryZ9R8dH-AHM"
      },
      "source": [
        "# Train student as doen usually\n",
        "student_scratch.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "# Train and evaluate student trained from scratch.\n",
        "student_scratch.fit(x_train, y_train, epochs=10)\n",
        "student_scratch.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZQHbZbI-AHN"
      },
      "source": [
        "If the teacher is trained for 5 full epochs and the student is distilled on this teacher\n",
        "for 3 full epochs, you should in this example experience a performance boost compared to\n",
        "training the same student model from scratch, and even compared to the teacher itself.\n",
        "You should expect the teacher to have accuracy around 97.6%, the student trained from\n",
        "scratch should be around 97.6%, and the distilled student should be around 98.1%. Remove\n",
        "or try out different seeds to use different weight initializations."
      ]
    }
  ]
}